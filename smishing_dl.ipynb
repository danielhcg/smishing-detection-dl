{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "k7zLjmVlNEn2",
        "tsrbu3D959UR",
        "o73gn1Fe6qsl",
        "7gwEocow6sGp",
        "L0tiWXmJ6s_R",
        "rTQavJq_6vjP",
        "Dh75EFmk7ob3",
        "MQ-mp4QyE87b",
        "aSnIkjEOFxKu",
        "9Yjy8_BfGv5K",
        "GCD5cNRLJwVI",
        "KJknPD8hKeLJ",
        "QBIUzQKeLB4y",
        "IIGQlHOiLki8",
        "9OBw3KhGMIFN",
        "uX_85m0bMnph",
        "72LdVXheazM0",
        "KpD9mfmBa9aO",
        "tUNOieE8bMZQ"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielhcg/smishing-detection-dl/blob/main/smishing_dl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Colab Shareable Link:\n",
        "https://colab.research.google.com/drive/1xWgjgZJHjXQ_lOvINxCHiT3bbgP0RDPK?usp=sharing\n"
      ],
      "metadata": {
        "id": "T2udKO-SANFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "kR9A9AnGMXay",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcdf42e8-7f67-4014-ff00-a0a913014416"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langid"
      ],
      "metadata": {
        "id": "xIytfWiNqr9g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a69f188f-a58e-4d14-d130-9afe87bb1055"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langid\n",
            "  Downloading langid-1.1.6.tar.gz (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from langid) (1.25.2)\n",
            "Building wheels for collected packages: langid\n",
            "  Building wheel for langid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langid: filename=langid-1.1.6-py3-none-any.whl size=1941172 sha256=7f2964e59cc806a444c6bb5436deaf788e21c04abcc47830d931b70879c7fd16\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/c8/c6/eed80894918490a175677414d40bd7c851413bbe03d4856c3c\n",
            "Successfully built langid\n",
            "Installing collected packages: langid\n",
            "Successfully installed langid-1.1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDZ2LohhcMpO"
      },
      "outputs": [],
      "source": [
        "# Load the dependencies and your data set.\n",
        "import keras\n",
        "from keras.datasets import imdb\n",
        "# from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn import preprocessing\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Dropout\n",
        "from keras.layers import Embedding, SpatialDropout1D, Conv1D, GlobalMaxPool1D, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import os\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from keras.layers import LSTM, Bidirectional # LSTM\n",
        "from keras.layers import GRU # GRU\n",
        "from keras.models import Model # Non sequential model\n",
        "from keras.layers import Input, concatenate # Non sequential model\n",
        "\n",
        "from keras.layers import SimpleRNN\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVmMAXx7kYRX"
      },
      "source": [
        "## Evaluating the data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "spam_dataset = pd.read_csv('gdrive/MyDrive/CS673 ANN/Project/Dataset/spam.csv')\n",
        "ham_dataset = pd.read_csv('gdrive/MyDrive/CS673 ANN/Project/Dataset/ham.csv')\n",
        "smishing_dataset = pd.read_csv('gdrive/MyDrive/CS673 ANN/Project/Dataset/smishing.csv')\n",
        "\n",
        "# Print the shape of the datasets\n",
        "print(\"Spam dataset shape:    \", spam_dataset.shape)\n",
        "print(\"Ham dataset shape:     \", ham_dataset.shape)\n",
        "print(\"Smishing dataset shape:\", smishing_dataset.shape)"
      ],
      "metadata": {
        "id": "7Yf5hN2iMpR7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "69a0bb53-7c48-4251-aa9a-ac7e76e982a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'gdrive/MyDrive/CS673 ANN/Project/Dataset/spam.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-085b37a6a3c7>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mspam_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gdrive/MyDrive/CS673 ANN/Project/Dataset/spam.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mham_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gdrive/MyDrive/CS673 ANN/Project/Dataset/ham.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msmishing_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gdrive/MyDrive/CS673 ANN/Project/Dataset/smishing.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'gdrive/MyDrive/CS673 ANN/Project/Dataset/spam.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Tweets spam\n",
        "tweet_2018 = pd.read_csv('gdrive/MyDrive/CS673 ANN/Project/Dataset/2018.csv')\n",
        "tweet_2019 = pd.read_csv('gdrive/MyDrive/CS673 ANN/Project/Dataset/2019.csv')\n",
        "tweet_2020 = pd.read_csv('gdrive/MyDrive/CS673 ANN/Project/Dataset/2020.csv')\n",
        "tweet_2021 = pd.read_csv('gdrive/MyDrive/CS673 ANN/Project/Dataset/2021.csv')\n",
        "tweet_2022 = pd.read_csv('gdrive/MyDrive/CS673 ANN/Project/Dataset/2022.csv')\n",
        "\n",
        "print(\"2018 dataset shape:\", tweet_2018.shape)\n",
        "print(\"2019 dataset shape:\", tweet_2019.shape)\n",
        "print(\"2020 dataset shape:\", tweet_2020.shape)\n",
        "print(\"2021 dataset shape:\", tweet_2021.shape)\n",
        "print(\"2022 dataset shape:\", tweet_2022.shape)"
      ],
      "metadata": {
        "id": "_qgE_822QVLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding new data of smishing\n",
        "new_smishing_dataset = pd.read_csv('gdrive/MyDrive/CS673 ANN/Project/smishing new dataset.csv')\n",
        "\n",
        "print(\"New Smishing dataset shape:\", new_smishing_dataset.shape)"
      ],
      "metadata": {
        "id": "KSNOi38SLd4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenating smishing\n",
        "smishing_concat = pd.concat([smishing_dataset, new_smishing_dataset], axis=0)\n",
        "smishing_concat.shape"
      ],
      "metadata": {
        "id": "bL-sBB7iLzLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenating tweets\n",
        "tweet_concat = pd.concat([tweet_2018, tweet_2019, tweet_2020, tweet_2021, tweet_2022], axis=0)\n",
        "tweet_concat.shape"
      ],
      "metadata": {
        "id": "h4zWOYEySeKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Total Spam concat\n",
        "spam = pd.concat([tweet_concat, spam_dataset], axis=0)\n",
        "spam.shape"
      ],
      "metadata": {
        "id": "TBLHoJpeS_ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filtering the data"
      ],
      "metadata": {
        "id": "eOqHNJ57ss5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing duplicates of smish\n",
        "smish_no_duplicates = smishing_concat.drop_duplicates()\n",
        "smish_no_duplicates.shape"
      ],
      "metadata": {
        "id": "CzVh8YhfLyOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing duplicates of spam\n",
        "spam_no_duplicates = spam.drop_duplicates()\n",
        "spam_no_duplicates.shape"
      ],
      "metadata": {
        "id": "O58u7IFNbRRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spam_no_duplicates.head()"
      ],
      "metadata": {
        "id": "aDyTrAUqq0bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import langid\n",
        "import pandas as pd\n",
        "\n",
        "# Function to identify the language of a text\n",
        "def identify_language(text):\n",
        "    try:\n",
        "        lang, _ = langid.classify(text)\n",
        "        return lang\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "# Add a new column 'language' to the DataFrame\n",
        "spam_no_duplicates['language'] = spam_no_duplicates['Text'].apply(identify_language)\n",
        "\n",
        "# Filter out non-English statements\n",
        "spam_no_duplicates_english = spam_no_duplicates[spam_no_duplicates['language'] == 'en']\n",
        "\n",
        "# Display the DataFrame with only English statements\n",
        "print(spam_no_duplicates_english.shape,'\\nDropping extra column')\n",
        "spam_final_dataset = spam_no_duplicates_english.drop('language', axis=1)\n",
        "spam_final_dataset.shape"
      ],
      "metadata": {
        "id": "EWNR5TmmqdJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing duplicates of ham\n",
        "ham_no_duplicates = ham_dataset.drop_duplicates()\n",
        "ham_no_duplicates.shape"
      ],
      "metadata": {
        "id": "b-79RVK4HZIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting the dataset"
      ],
      "metadata": {
        "id": "WA5vSHQ2sxCu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5ufLEbsfdCG"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Combine all datasets into one DataFrame\n",
        "all_data = pd.concat([spam_final_dataset, ham_no_duplicates, smish_no_duplicates], ignore_index=True)\n",
        "\n",
        "# Combine similar labels and replace 'ham' with 'Ham'\n",
        "all_data['Label'] = all_data['Label'].replace({'spam': 'Spam', 'smishing': 'Smishing', 'ham': 'Ham'})\n",
        "\n",
        "X = all_data['Text']\n",
        "y = all_data['Label']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Display the shapes of the resulting datasets\n",
        "print(\"X_train shape:\", x_train.shape)\n",
        "print(\"X_test shape:\", x_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yANTZVMBfnyG"
      },
      "outputs": [],
      "source": [
        "x_train[400:410]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1PZxvALfuBG"
      },
      "outputs": [],
      "source": [
        "y_train[400:410]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45eNB_Phl0uB"
      },
      "outputs": [],
      "source": [
        "y_train.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZn3dUUc0b-C"
      },
      "outputs": [],
      "source": [
        "label_counts = y_train.value_counts()\n",
        "label_counts.plot(kind='bar')\n",
        "plt.title('Distribution of Labels in y_train')\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JR3jy-ANltPo"
      },
      "outputs": [],
      "source": [
        "y_test.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "va1mNPtn0hCS"
      },
      "outputs": [],
      "source": [
        "label_counts = y_test.value_counts()\n",
        "label_counts.plot(kind='bar')\n",
        "plt.title('Distribution of Labels in y_val')\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyper Parameters setting\n"
      ],
      "metadata": {
        "id": "R8W_7V9r888x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4\n",
        "#    Set your hyperparameters:\n",
        "\n",
        "#    epochs,\n",
        "#    batch size,\n",
        "#    dimension,\n",
        "#    unique words,\n",
        "#    stop words,\n",
        "#    maximum message length,\n",
        "#    padding type,\n",
        "#    truncation type,\n",
        "#    #dense layer,\n",
        "#    dropout,\n",
        "#    n_filters,\n",
        "#    optimizers,\n",
        "#    or anything else needed.\n",
        "\n",
        "num_classes= 3\n",
        "\n",
        "output_dir = 'model_output/dense'\n",
        "epochs = 7\n",
        "batch_size = 128\n",
        "\n",
        "n_dim = 100\n",
        "n_unique_words = 5000\n",
        "total_elements = 5405\n",
        "n_words_to_skip = 50\n",
        "pad_type = trunc_type = 'pre'\n",
        "n_dense = 100\n",
        "dropout = 0.5\n",
        "\n",
        "#CNN\n",
        "cnn_max_review =100\n",
        "cnn_dense = 256\n",
        "cnn_dropout = 0.2\n",
        "n_conv = 256\n",
        "k_conv = 3\n",
        "\n",
        "#RNN\n",
        "n_rnn = 256\n",
        "drop_rnn = 0.2\n",
        "drop_embed = 0.2\n",
        "\n",
        "#LSTM\n",
        "out_put_ = 'model_output/conv'\n",
        "lstm_max_review = 100\n",
        "lstm_drop_embed = 0.2\n",
        "lstm_n_conv = 256\n",
        "lstm_k_conv = 0.2\n",
        "lstm_dropout = 0.2"
      ],
      "metadata": {
        "id": "3r0KvUeRc7Bg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM layer architecture:\n",
        "n_lstm = 256\n",
        "drop_lstm = 0.2\n",
        "\n",
        "# CNN architecture\n",
        "n_conv = 64\n",
        "k_conv = 3\n",
        "mp_size=4\n",
        "\n",
        "# GRU architecture\n",
        "n_gru = 256\n",
        "drop_gru = 0.2\n",
        "\n",
        "# LSTM stack layer architecture\n",
        "n_lstm_1 = 64\n",
        "n_lstm_2 = 64\n",
        "drop_lstm = 0.2\n",
        "\n",
        "# Multi conv (Non-sequential)\n",
        "n_conv_1 = n_conv_2 = n_conv_3 = 256\n",
        "k_conv_1 = 3\n",
        "k_conv_2 = 2\n",
        "k_conv_3 = 4"
      ],
      "metadata": {
        "id": "iE_dkduPJz7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing\n"
      ],
      "metadata": {
        "id": "0KSZ15SSsmaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3\n",
        "# Apply all ( or subset of) the preprocessing steps that we learned in the class:\n",
        "# Tokenization\n",
        "# Convertion to lowercase\n",
        "# Removing stop words\n",
        "# Removing puntuation\n",
        "# Stemming\n",
        "# n-grams\n",
        "\n",
        "max_message_length = 100\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vect = CountVectorizer()\n",
        "vect.fit(x_train)\n",
        "\n",
        "# Tokenizing\n",
        "token = Tokenizer(num_words=n_unique_words)\n",
        "token.fit_on_texts(x_train)\n",
        "\n",
        "x_train_emb = token.texts_to_sequences(x_train)\n",
        "x_test_emb = token.texts_to_sequences(x_test)\n",
        "\n",
        "print(x_train[0])\n",
        "print(x_train_emb[0])\n",
        "\n",
        "x_train = pad_sequences(x_train_emb, padding='pre', maxlen=max_message_length)\n",
        "x_test = pad_sequences(x_test_emb, padding='pre', maxlen=max_message_length)\n",
        "\n",
        "print(x_train[1])"
      ],
      "metadata": {
        "id": "uLUYOqZOczUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_original = y_train\n",
        "y_test_original = y_test"
      ],
      "metadata": {
        "id": "QzZQgL8K_--d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(y_train)\n",
        "y_test = label_encoder.transform(y_test)"
      ],
      "metadata": {
        "id": "Um0WgbqXpqUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1 Dense"
      ],
      "metadata": {
        "id": "4EytVCl44bKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Output Directory name:\n",
        "output_dir = 'model_output/dense' # Store model's parameter after each epoch, useful at a later time\n",
        "\n",
        "# If the output_dir directory doesn't exist, we use the mkdirs() method to make it\n",
        "if not os.path.exists(output_dir):\n",
        "  os.makedirs(output_dir)\n",
        "\n",
        "# Allow to save our model parameters after each epoch during training\n",
        "modelcheckpoint = ModelCheckpoint(filepath = output_dir + '/weights.{epoch:02d}.hdf5')"
      ],
      "metadata": {
        "id": "Zu2f2PNKNJb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding, SpatialDropout1D, Conv1D, GlobalMaxPool1D, SimpleRNN, LSTM, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "# Dense\n",
        "model1 = Sequential()\n",
        "model1.add(Embedding(n_unique_words, n_dim, input_length=max_message_length))\n",
        "model1.add(Flatten())\n",
        "model1.add(Dense(n_dense, activation='relu'))\n",
        "model1.add(Dropout(dropout))\n",
        "model1.add(Dense(n_dense, activation='relu'))\n",
        "model1.add(Dropout(dropout))\n",
        "model1.add(Dense(num_classes, activation='softmax'))  # Change to num_classes\n",
        "\n",
        "# Compile\n",
        "model1.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model1.summary()\n",
        "\n",
        "start_time = time.time()\n",
        "history1 = model1.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test), callbacks=[modelcheckpoint])\n",
        "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed"
      ],
      "metadata": {
        "id": "SRtlquVzdEBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7zLjmVlNEn2"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqngDOVbS3H4"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "# Visualizing training and validation accuracy for this Model\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history1.history['accuracy'])\n",
        "plt.plot(history1.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Visualizing training and validation loss for this Model\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history1.history['loss'])\n",
        "plt.plot(history1.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss/Error')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPJ6WfEaNEn3"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Evaluating the model\n",
        "y_pred_prob = model1.predict(x_test)\n",
        "y_pred = y_pred_prob.argmax(axis=-1)\n",
        "\n",
        "# Calculating accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {100*accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Iy9EJVPNEn4"
      },
      "outputs": [],
      "source": [
        "# Plot ROC curve for model\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(y_pred_prob)\n",
        "_ = plt.axvline(x=0.5, color='orange')\n",
        "plt.title(\"Model ROC-AUC\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRIZI96UNEn5"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming y_test_encoded and y_pred_prob are defined\n",
        "# y_test_encoded: True labels (encoded, not one-hot encoded)\n",
        "# y_pred_prob: Predicted probabilities for each class\n",
        "\n",
        "# Calculate ROC-AUC score for model (one-vs-all approach for multi-class)\n",
        "roc_auc = []\n",
        "\n",
        "# Create a figure and axis\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot ROC curve for each class\n",
        "for i in range(num_classes):\n",
        "    fpr, tpr, _ = roc_curve(y_test== i, y_pred_prob[:, i])\n",
        "    roc_auc_class = auc(fpr, tpr)\n",
        "    roc_auc.append(roc_auc_class * 100.0)\n",
        "    plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc_class:.2f})')\n",
        "\n",
        "# Set plot title and labels\n",
        "plt.title(\"Model ROC Curves for Multi-class Classification\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 2 CNN"
      ],
      "metadata": {
        "id": "7ODJ4lYJ4kX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Output Directory name:\n",
        "output_dir = 'model_output/CNN' # Store model's parameter after each epoch, useful at a later time\n",
        "\n",
        "# Allow to save our model parameters after each epoch during training\n",
        "modelcheckpoint = ModelCheckpoint(filepath = output_dir + '/weights.{epoch:02d}.hdf5')\n",
        "\n",
        "# If the output_dir directory doesn't exist, we use the mkdirs() method to make it\n",
        "if not os.path.exists(output_dir):\n",
        "  os.makedirs(output_dir)"
      ],
      "metadata": {
        "id": "e78S4CalNdtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN\n",
        "model2 = Sequential()\n",
        "model2.add(Embedding(n_unique_words, n_dim, input_length=cnn_max_review))\n",
        "model2.add(SpatialDropout1D(drop_embed))\n",
        "model2.add(Conv1D(n_conv, k_conv, activation='relu'))\n",
        "model2.add(GlobalMaxPool1D())\n",
        "model2.add(Dense(cnn_dense, activation='relu'))\n",
        "model2.add(Dropout(cnn_dropout))\n",
        "model2.add(Dense(num_classes, activation='softmax'))  # Change to num_classes\n",
        "model2.summary()\n",
        "\n",
        "model2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "start_time = time.time()\n",
        "history2 = model2.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test), callbacks=[modelcheckpoint])\n",
        "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed"
      ],
      "metadata": {
        "id": "TcthdlN1lmzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsrbu3D959UR"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5VVJ_LY59UT"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "# Visualizing training and validation accuracy for this Model\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history2.history['accuracy'])\n",
        "plt.plot(history2.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Visualizing training and validation loss for this Model\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history2.history['loss'])\n",
        "plt.plot(history2.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss/Error')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7lAEwdn59UV"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Evaluating the model\n",
        "y_pred_prob = model2.predict(x_test)\n",
        "y_pred = y_pred_prob.argmax(axis=-1)\n",
        "\n",
        "# Calculating accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {100*accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "On7MCOzW59UW"
      },
      "outputs": [],
      "source": [
        "# Plot ROC curve for model\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(y_pred_prob)\n",
        "_ = plt.axvline(x=0.5, color='orange')\n",
        "plt.title(\"Model ROC-AUC\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jrZXihe59UX"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming y_test_encoded and y_pred_prob are defined\n",
        "# y_test_encoded: True labels (encoded, not one-hot encoded)\n",
        "# y_pred_prob: Predicted probabilities for each class\n",
        "\n",
        "# Calculate ROC-AUC score for model (one-vs-all approach for multi-class)\n",
        "roc_auc = []\n",
        "\n",
        "# Create a figure and axis\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot ROC curve for each class\n",
        "for i in range(num_classes):\n",
        "    fpr, tpr, _ = roc_curve(y_test== i, y_pred_prob[:, i])\n",
        "    roc_auc_class = auc(fpr, tpr)\n",
        "    roc_auc.append(roc_auc_class * 100.0)\n",
        "    plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc_class:.2f})')\n",
        "\n",
        "# Set plot title and labels\n",
        "plt.title(\"Model ROC Curves for Multi-class Classification\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 3 RNN"
      ],
      "metadata": {
        "id": "df09h5OH4pDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Output Directory name:\n",
        "output_dir = 'model_output/Simple RNN' # Store model's parameter after each epoch, useful at a later time\n",
        "\n",
        "# If the output_dir directory doesn't exist, we use the mkdirs() method to make it\n",
        "if not os.path.exists(output_dir):\n",
        "  os.makedirs(output_dir)\n",
        "\n",
        "# Allow to save our model parameters after each epoch during training\n",
        "modelcheckpoint = ModelCheckpoint(filepath = output_dir + '/weights.{epoch:02d}.hdf5')\n",
        "\n",
        "drop_embed = 0.2\n",
        "n_rnn = 256\n",
        "drop_rnn = 0.2\n",
        "#n_dense = 256\n",
        "#dropout = 0.2"
      ],
      "metadata": {
        "id": "FBmUlMkFNg2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN\n",
        "model3 = Sequential()\n",
        "model3.add(Embedding(n_unique_words, n_dim, input_length=max_message_length))\n",
        "model3.add(SpatialDropout1D(drop_embed))\n",
        "model3.add(SimpleRNN(n_rnn, dropout=drop_rnn))\n",
        "model3.add(Dense(num_classes, activation='softmax'))  # Change to num_classes\n",
        "\n",
        "model3.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model3.summary()\n",
        "\n",
        "start_time = time.time()\n",
        "history3 = model3.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test), callbacks=[modelcheckpoint])\n",
        "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed"
      ],
      "metadata": {
        "id": "zmUwZdgYls0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o73gn1Fe6qsl"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4bPvGqR6qs_"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "# Visualizing training and validation accuracy for this Model\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history3.history['accuracy'])\n",
        "plt.plot(history3.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Visualizing training and validation loss for this Model\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history3.history['loss'])\n",
        "plt.plot(history3.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss/Error')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYY1Mp8L6qtA"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Evaluating the model\n",
        "y_pred_prob = model3.predict(x_test)\n",
        "y_pred = y_pred_prob.argmax(axis=-1)\n",
        "\n",
        "# Calculating accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {100*accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXAZF5fs6qtA"
      },
      "outputs": [],
      "source": [
        "# Plot ROC curve for model\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(y_pred_prob)\n",
        "_ = plt.axvline(x=0.5, color='orange')\n",
        "plt.title(\"Model ROC-AUC\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlf3AZut6qtB"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming y_test_encoded and y_pred_prob are defined\n",
        "# y_test_encoded: True labels (encoded, not one-hot encoded)\n",
        "# y_pred_prob: Predicted probabilities for each class\n",
        "\n",
        "# Calculate ROC-AUC score for model (one-vs-all approach for multi-class)\n",
        "roc_auc = []\n",
        "\n",
        "# Create a figure and axis\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot ROC curve for each class\n",
        "for i in range(num_classes):\n",
        "    fpr, tpr, _ = roc_curve(y_test== i, y_pred_prob[:, i])\n",
        "    roc_auc_class = auc(fpr, tpr)\n",
        "    roc_auc.append(roc_auc_class * 100.0)\n",
        "    plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc_class:.2f})')\n",
        "\n",
        "# Set plot title and labels\n",
        "plt.title(\"Model ROC Curves for Multi-class Classification\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 4 LSTM"
      ],
      "metadata": {
        "id": "D3gunGh_4r3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Output Directory name:\n",
        "output_dir = 'model_output/LSTM' # Store model's parameter after each epoch, useful at a later time\n",
        "\n",
        "# If the output_dir directory doesn't exist, we use the mkdirs() method to make it\n",
        "if not os.path.exists(output_dir):\n",
        "  os.makedirs(output_dir)\n",
        "\n",
        "# Allow to save our model parameters after each epoch during training\n",
        "modelcheckpoint = ModelCheckpoint(filepath = output_dir + '/weights.{epoch:02d}.hdf5')\n"
      ],
      "metadata": {
        "id": "n6d7r1QsNkaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM\n",
        "model4 = Sequential()\n",
        "model4.add(Embedding(n_unique_words, n_dim, input_length=lstm_max_review))\n",
        "model4.add(SpatialDropout1D(lstm_drop_embed))\n",
        "model4.add(LSTM(n_lstm, dropout=lstm_dropout))\n",
        "model4.add(Dense(num_classes, activation='softmax'))  # Change to num_classes\n",
        "\n",
        "model4.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model4.summary()\n",
        "\n",
        "start_time = time.time()\n",
        "history4 = model4.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test), callbacks=[modelcheckpoint])\n",
        "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed"
      ],
      "metadata": {
        "id": "N6co7xzXlwP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gwEocow6sGp"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adyZAbcC6sHH"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "# Visualizing training and validation accuracy for this Model\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history4.history['accuracy'])\n",
        "plt.plot(history4.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Visualizing training and validation loss for this Model\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history4.history['loss'])\n",
        "plt.plot(history4.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss/Error')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbaRfYIy6sHJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "# Output Directory name:\n",
        "output_dir = 'model_output/LSTM'\n",
        "# Evaluating the model\n",
        "model4.load_weights(output_dir + \"/weights.04.hdf5\")\n",
        "y_pred_prob = model4.predict(x_test)\n",
        "y_pred = y_pred_prob.argmax(axis=-1)\n",
        "\n",
        "# Calculating accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {100*accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6OWSzlv6sHJ"
      },
      "outputs": [],
      "source": [
        "# Plot ROC curve for model\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(y_pred_prob)\n",
        "_ = plt.axvline(x=0.5, color='orange')\n",
        "plt.title(\"Model ROC-AUC\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4AM5uvW6sHK"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming y_test_encoded and y_pred_prob are defined\n",
        "# y_test_encoded: True labels (encoded, not one-hot encoded)\n",
        "# y_pred_prob: Predicted probabilities for each class\n",
        "\n",
        "# Calculate ROC-AUC score for model (one-vs-all approach for multi-class)\n",
        "roc_auc = []\n",
        "\n",
        "# Create a figure and axis\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot ROC curve for each class\n",
        "for i in range(num_classes):\n",
        "    fpr, tpr, _ = roc_curve(y_test== i, y_pred_prob[:, i])\n",
        "    roc_auc_class = auc(fpr, tpr)\n",
        "    roc_auc.append(roc_auc_class * 100.0)\n",
        "    plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc_class:.2f})')\n",
        "\n",
        "# Set plot title and labels\n",
        "plt.title(\"Model ROC Curves for Multi-class Classification\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 5 Improved CNN"
      ],
      "metadata": {
        "id": "ypRhB_E94u0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Output Directory name:\n",
        "output_dir = 'model_output/Improved-CNN' # Store model's parameter after each epoch, useful at a later time\n",
        "\n",
        "# If the output_dir directory doesn't exist, we use the mkdirs() method to make it\n",
        "if not os.path.exists(output_dir):\n",
        "  os.makedirs(output_dir)\n",
        "\n",
        "# Allow to save our model parameters after each epoch during training\n",
        "modelcheckpoint = ModelCheckpoint(filepath = output_dir + '/weights.{epoch:02d}.hdf5')"
      ],
      "metadata": {
        "id": "79Ov_g_SNoQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#    Tune your model\n",
        "# Select the model which performed the best in the previous step and then tune the hyperparameters\n",
        "# Improved CNN\n",
        "model5 = Sequential()\n",
        "\n",
        "model5.add(Embedding(n_unique_words, n_dim, input_length=cnn_max_review))\n",
        "model5.add(SpatialDropout1D(drop_embed))\n",
        "\n",
        "model5.add(Conv1D(n_conv,k_conv,activation = 'relu'))\n",
        "model5.add(GlobalMaxPool1D())\n",
        "\n",
        "model5.add(Dense(64, activation='relu'))\n",
        "model5.add(Dense(128,activation='relu'))\n",
        "model5.add(Dropout(0.4))\n",
        "model5.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model5.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model5.summary()\n",
        "\n",
        "start_time = time.time()\n",
        "history5 =  model5.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test), callbacks=[modelcheckpoint])\n",
        "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed"
      ],
      "metadata": {
        "id": "c2MsqF3FdQLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0tiWXmJ6s_R"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1dX75kb6s_S"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "# Visualizing training and validation accuracy for this Model\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history5.history['accuracy'])\n",
        "plt.plot(history5.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Visualizing training and validation loss for this Model\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history5.history['loss'])\n",
        "plt.plot(history5.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss/Error')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uifMF4gQ6s_T"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Evaluating the model\n",
        "y_pred_prob = model5.predict(x_test)\n",
        "y_pred = y_pred_prob.argmax(axis=-1)\n",
        "\n",
        "# Calculating accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {100*accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fu2rKWU6s_U"
      },
      "outputs": [],
      "source": [
        "# Plot ROC curve for model\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(y_pred_prob)\n",
        "_ = plt.axvline(x=0.5, color='orange')\n",
        "plt.title(\"Model ROC-AUC\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1kGnCgA6s_V"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming y_test_encoded and y_pred_prob are defined\n",
        "# y_test_encoded: True labels (encoded, not one-hot encoded)\n",
        "# y_pred_prob: Predicted probabilities for each class\n",
        "\n",
        "# Calculate ROC-AUC score for model (one-vs-all approach for multi-class)\n",
        "roc_auc = []\n",
        "\n",
        "# Create a figure and axis\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot ROC curve for each class\n",
        "for i in range(num_classes):\n",
        "    fpr, tpr, _ = roc_curve(y_test== i, y_pred_prob[:, i])\n",
        "    roc_auc_class = auc(fpr, tpr)\n",
        "    roc_auc.append(roc_auc_class * 100.0)\n",
        "    plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc_class:.2f})')\n",
        "\n",
        "# Set plot title and labels\n",
        "plt.title(\"Model ROC Curves for Multi-class Classification\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 6 GRU"
      ],
      "metadata": {
        "id": "bCC1_wEj43ct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Output Directory name:\n",
        "output_dir = 'model_output/GRU' # Store model's parameter after each epoch, useful at a later time\n",
        "\n",
        "# If the output_dir directory doesn't exist, we use the mkdirs() method to make it\n",
        "if not os.path.exists(output_dir):\n",
        "  os.makedirs(output_dir)\n",
        "\n",
        "# Allow to save our model parameters after each epoch during training\n",
        "modelcheckpoint = ModelCheckpoint(filepath = output_dir + '/weights.{epoch:02d}.hdf5')"
      ],
      "metadata": {
        "id": "kEkLI4EGNsSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GRU\n",
        "model6 = Sequential()\n",
        "model6.add(Embedding(n_unique_words, n_dim, input_length=max_message_length))\n",
        "model6.add(SpatialDropout1D(drop_embed))\n",
        "model6.add(GRU(n_gru, dropout=drop_gru))\n",
        "model6.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model6.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model6.summary()\n",
        "\n",
        "start_time = time.time()\n",
        "historyGRU =  model6.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test), callbacks=[modelcheckpoint])\n",
        "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed"
      ],
      "metadata": {
        "id": "4twBOHuGJTT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTQavJq_6vjP"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4npOTKlL6vkC"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "# Visualizing training and validation accuracy for this Model\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(historyGRU.history['accuracy'])\n",
        "plt.plot(historyGRU.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Visualizing training and validation loss for this Model\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(historyGRU.history['loss'])\n",
        "plt.plot(historyGRU.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss/Error')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PP11R34M6vkD"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Evaluating the model\n",
        "y_pred_prob = model6.predict(x_test)\n",
        "y_pred = y_pred_prob.argmax(axis=-1)\n",
        "\n",
        "# Calculating accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {100*accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnsg4l3w6vkD"
      },
      "outputs": [],
      "source": [
        "# Plot ROC curve for model\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(y_pred_prob)\n",
        "_ = plt.axvline(x=0.5, color='orange')\n",
        "plt.title(\"Model ROC-AUC\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tH7J5atu6vkE"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming y_test_encoded and y_pred_prob are defined\n",
        "# y_test_encoded: True labels (encoded, not one-hot encoded)\n",
        "# y_pred_prob: Predicted probabilities for each class\n",
        "\n",
        "# Calculate ROC-AUC score for model (one-vs-all approach for multi-class)\n",
        "roc_auc = []\n",
        "\n",
        "# Create a figure and axis\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot ROC curve for each class\n",
        "for i in range(num_classes):\n",
        "    fpr, tpr, _ = roc_curve(y_test== i, y_pred_prob[:, i])\n",
        "    roc_auc_class = auc(fpr, tpr)\n",
        "    roc_auc.append(roc_auc_class * 100.0)\n",
        "    plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc_class:.2f})')\n",
        "\n",
        "# Set plot title and labels\n",
        "plt.title(\"Model ROC Curves for Multi-class Classification\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 7 Bi-LSTM"
      ],
      "metadata": {
        "id": "xOLkCViv49ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Output Directory name:\n",
        "output_dir = 'model_output/Bi-LSTM' # Store model's parameter after each epoch, useful at a later time\n",
        "\n",
        "# If the output_dir directory doesn't exist, we use the mkdirs() method to make it\n",
        "if not os.path.exists(output_dir):\n",
        "  os.makedirs(output_dir)\n",
        "\n",
        "# Allow to save our model parameters after each epoch during training\n",
        "modelcheckpoint = ModelCheckpoint(filepath = output_dir + '/weights.{epoch:02d}.hdf5')"
      ],
      "metadata": {
        "id": "4PkgD-XUNvhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7\n",
        "model7 = Sequential()\n",
        "model7.add(Embedding(n_unique_words, n_dim, input_length=max_message_length))\n",
        "model7.add(SpatialDropout1D(drop_embed))\n",
        "model7.add(Bidirectional(LSTM(n_lstm_1, dropout=0.4, return_sequences=True)))\n",
        "model7.add(Bidirectional(LSTM(n_lstm_2, dropout=0.4)))\n",
        "model7.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model7.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model7.summary()\n",
        "\n",
        "start_time = time.time()\n",
        "history7 =  model7.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test), callbacks=[modelcheckpoint])\n",
        "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed"
      ],
      "metadata": {
        "id": "jVXVxddKCyJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dh75EFmk7ob3"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmtLCQ4o7ocE"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "# Visualizing training and validation accuracy for this Model\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history7.history['accuracy'])\n",
        "plt.plot(history7.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Visualizing training and validation loss for this Model\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history7.history['loss'])\n",
        "plt.plot(history7.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss/Error')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_r9cDiL7ocF"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Evaluating the model\n",
        "y_pred_prob = model7.predict(x_test)\n",
        "y_pred = y_pred_prob.argmax(axis=-1)\n",
        "\n",
        "# Calculating accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {100*accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vw-BPbrJ7ocF"
      },
      "outputs": [],
      "source": [
        "# Plot ROC curve for model\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(y_pred_prob)\n",
        "_ = plt.axvline(x=0.5, color='orange')\n",
        "plt.title(\"Model ROC-AUC\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zOLlVMf7ocF"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming y_test_encoded and y_pred_prob are defined\n",
        "# y_test_encoded: True labels (encoded, not one-hot encoded)\n",
        "# y_pred_prob: Predicted probabilities for each class\n",
        "\n",
        "# Calculate ROC-AUC score for model (one-vs-all approach for multi-class)\n",
        "roc_auc = []\n",
        "\n",
        "# Create a figure and axis\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot ROC curve for each class\n",
        "for i in range(num_classes):\n",
        "    fpr, tpr, _ = roc_curve(y_test== i, y_pred_prob[:, i])\n",
        "    roc_auc_class = auc(fpr, tpr)\n",
        "    roc_auc.append(roc_auc_class * 100.0)\n",
        "    plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc_class:.2f})')\n",
        "\n",
        "# Set plot title and labels\n",
        "plt.title(\"Model ROC Curves for Multi-class Classification\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting y_train and y_test to one_hot"
      ],
      "metadata": {
        "id": "xfsg-msp_vb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train_original)\n",
        "y_test_encoded = label_encoder.transform(y_test_original)\n",
        "\n",
        "# Convert integer labels to one-hot encoding\n",
        "y_train_one_hot = to_categorical(y_train_encoded)\n",
        "y_test_one_hot = to_categorical(y_test_encoded)\n",
        "\n",
        "# Check the shapes\n",
        "print(y_train_one_hot.shape, y_test_one_hot.shape)"
      ],
      "metadata": {
        "id": "7ZE_loM4omrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 8 Stacked LSTM"
      ],
      "metadata": {
        "id": "Qa3DGg_l8RQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Output Directory name:\n",
        "output_dir = 'model_output/Stacked-LSTM' # Store model's parameter after each epoch, useful at a later time\n",
        "\n",
        "# If the output_dir directory doesn't exist, we use the mkdirs() method to make it\n",
        "if not os.path.exists(output_dir):\n",
        "  os.makedirs(output_dir)\n",
        "\n",
        "# Allow to save our model parameters after each epoch during training\n",
        "modelcheckpoint = ModelCheckpoint(filepath = output_dir + '/weights.{epoch:02d}.hdf5')"
      ],
      "metadata": {
        "id": "Olk7KVNtNyNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model8 = Sequential()\n",
        "model8.add(Embedding(n_unique_words, n_dim, input_length=max_message_length))\n",
        "model8.add(LSTM(100, return_sequences=True))\n",
        "model8.add(Dropout(dropout))\n",
        "model8.add(LSTM(100))\n",
        "model8.add(Dropout(dropout))\n",
        "model8.add(Dense(num_classes, activation='softmax'))  # Adjust the activation function\n",
        "model8.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model8.summary()\n",
        "\n",
        "start_time = time.time()\n",
        "history8 = model8.fit(x_train, y_train_one_hot, validation_data=(x_test, y_test_one_hot), epochs=epochs, batch_size=batch_size, callbacks=[modelcheckpoint])\n",
        "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed"
      ],
      "metadata": {
        "id": "5crF0hbS-p2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ-mp4QyE87b"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGnRc4b4E87u"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "# Visualizing training and validation accuracy for this Model\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history8.history['accuracy'])\n",
        "plt.plot(history8.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Visualizing training and validation loss for this Model\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history8.history['loss'])\n",
        "plt.plot(history8.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss/Error')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_24DJkyE87v"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Evaluating the model\n",
        "y_pred_prob = model8.predict(x_test)\n",
        "y_pred = y_pred_prob.argmax(axis=-1)\n",
        "\n",
        "# Calculating accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {100*accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2uhlBnHE87v"
      },
      "outputs": [],
      "source": [
        "# Plot ROC curve for model\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(y_pred_prob)\n",
        "_ = plt.axvline(x=0.5, color='orange')\n",
        "plt.title(\"Model ROC-AUC\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NTvzP3EE87v"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming y_test_encoded and y_pred_prob are defined\n",
        "# y_test_encoded: True labels (encoded, not one-hot encoded)\n",
        "# y_pred_prob: Predicted probabilities for each class\n",
        "\n",
        "# Calculate ROC-AUC score for model (one-vs-all approach for multi-class)\n",
        "roc_auc = []\n",
        "\n",
        "# Create a figure and axis\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot ROC curve for each class\n",
        "for i in range(num_classes):\n",
        "    fpr, tpr, _ = roc_curve(y_test== i, y_pred_prob[:, i])\n",
        "    roc_auc_class = auc(fpr, tpr)\n",
        "    roc_auc.append(roc_auc_class * 100.0)\n",
        "    plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc_class:.2f})')\n",
        "\n",
        "# Set plot title and labels\n",
        "plt.title(\"Model ROC Curves for Multi-class Classification\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 9 Conv-LSTM"
      ],
      "metadata": {
        "id": "KstQWZxXBQDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Output Directory name:\n",
        "output_dir = 'model_output/CNN-LSTM' # Store model's parameter after each epoch, useful at a later time\n",
        "\n",
        "# If the output_dir directory doesn't exist, we use the mkdirs() method to make it\n",
        "if not os.path.exists(output_dir):\n",
        "  os.makedirs(output_dir)\n",
        "\n",
        "# Allow to save our model parameters after each epoch during training\n",
        "modelcheckpoint = ModelCheckpoint(filepath = output_dir + '/weights.{epoch:02d}.hdf5')"
      ],
      "metadata": {
        "id": "-Z9MYSw6N0pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model9 = Sequential()\n",
        "model9.add(Embedding(n_unique_words, n_dim, input_length=max_message_length))\n",
        "model9.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "model9.add(MaxPooling1D(pool_size=2))\n",
        "model9.add(LSTM(100))\n",
        "model9.add(Dropout(dropout))\n",
        "model9.add(Dense(num_classes, activation='softmax'))  # Adjust the activation function\n",
        "model9.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model9.summary()\n",
        "\n",
        "start_time = time.time()\n",
        "history9 = model9.fit(x_train, y_train_one_hot, validation_data=(x_test, y_test_one_hot), epochs=epochs, batch_size=batch_size, callbacks=[modelcheckpoint])\n",
        "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed"
      ],
      "metadata": {
        "id": "qLLOK0tZBPML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSnIkjEOFxKu"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnUC_0ogFxLS"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "# Visualizing training and validation accuracy for this Model\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history9.history['accuracy'])\n",
        "plt.plot(history9.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Visualizing training and validation loss for this Model\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history9.history['loss'])\n",
        "plt.plot(history9.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss/Error')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVfswV3qFxLT"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Evaluating the model\n",
        "y_pred_prob = model9.predict(x_test)\n",
        "y_pred = y_pred_prob.argmax(axis=-1)\n",
        "\n",
        "# Calculating accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {100*accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNRnueAZFxLT"
      },
      "outputs": [],
      "source": [
        "# Plot ROC curve for model\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(y_pred_prob)\n",
        "_ = plt.axvline(x=0.5, color='orange')\n",
        "plt.title(\"Model ROC-AUC\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCQ8oQfhFxLU"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming y_test_encoded and y_pred_prob are defined\n",
        "# y_test_encoded: True labels (encoded, not one-hot encoded)\n",
        "# y_pred_prob: Predicted probabilities for each class\n",
        "\n",
        "# Calculate ROC-AUC score for model (one-vs-all approach for multi-class)\n",
        "roc_auc = []\n",
        "\n",
        "# Create a figure and axis\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot ROC curve for each class\n",
        "for i in range(num_classes):\n",
        "    fpr, tpr, _ = roc_curve(y_test== i, y_pred_prob[:, i])\n",
        "    roc_auc_class = auc(fpr, tpr)\n",
        "    roc_auc.append(roc_auc_class * 100.0)\n",
        "    plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc_class:.2f})')\n",
        "\n",
        "# Set plot title and labels\n",
        "plt.title(\"Model ROC Curves for Multi-class Classification\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 10 Multi-ConvNet"
      ],
      "metadata": {
        "id": "e08isL9oF33O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Output Directory name:\n",
        "output_dir = 'model_output/Multi-CovNet' # Store model's parameter after each epoch, useful at a later time\n",
        "\n",
        "# If the output_dir directory doesn't exist, we use the mkdirs() method to make it\n",
        "if not os.path.exists(output_dir):\n",
        "  os.makedirs(output_dir)\n",
        "\n",
        "# Allow to save our model parameters after each epoch during training\n",
        "modelcheckpoint = ModelCheckpoint(filepath = output_dir + '/weights.{epoch:02d}.hdf5')"
      ],
      "metadata": {
        "id": "7Vk3BvjlN3nC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model10 = Sequential()\n",
        "model10.add(Embedding(n_unique_words, n_dim, input_length=max_message_length))\n",
        "model10.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
        "model10.add(GlobalMaxPooling1D())\n",
        "model10.add(Dropout(dropout))\n",
        "model10.add(Dense(512, activation='relu'))\n",
        "model10.add(Dropout(dropout))\n",
        "model10.add(Dense(num_classes, activation='softmax'))\n",
        "model10.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model10.summary()\n",
        "\n",
        "start_time = time.time()\n",
        "history10 = model10.fit(x_train, y_train_one_hot, validation_data=(x_test, y_test_one_hot), epochs=epochs, batch_size=batch_size, callbacks=[modelcheckpoint])\n",
        "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed"
      ],
      "metadata": {
        "id": "4DRQmu99F9Ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Yjy8_BfGv5K"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzfFhklbGv5k"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "# Visualizing training and validation accuracy for this Model\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history10.history['accuracy'])\n",
        "plt.plot(history10.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Visualizing training and validation loss for this Model\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history10.history['loss'])\n",
        "plt.plot(history10.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss/Error')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcYnZLlIGv5l"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Evaluating the model\n",
        "y_pred_prob = model10.predict(x_test)\n",
        "y_pred = y_pred_prob.argmax(axis=-1)\n",
        "\n",
        "# Calculating accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {100*accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5BMG2KvGv5m"
      },
      "outputs": [],
      "source": [
        "# Plot ROC curve for model\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(y_pred_prob)\n",
        "_ = plt.axvline(x=0.5, color='orange')\n",
        "plt.title(\"Model ROC-AUC\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhX4uyF9Gv5n"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming y_test_encoded and y_pred_prob are defined\n",
        "# y_test_encoded: True labels (encoded, not one-hot encoded)\n",
        "# y_pred_prob: Predicted probabilities for each class\n",
        "\n",
        "# Calculate ROC-AUC score for model (one-vs-all approach for multi-class)\n",
        "roc_auc = []\n",
        "\n",
        "# Create a figure and axis\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot ROC curve for each class\n",
        "for i in range(num_classes):\n",
        "    fpr, tpr, _ = roc_curve(y_test== i, y_pred_prob[:, i])\n",
        "    roc_auc_class = auc(fpr, tpr)\n",
        "    roc_auc.append(roc_auc_class * 100.0)\n",
        "    plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc_class:.2f})')\n",
        "\n",
        "# Set plot title and labels\n",
        "plt.title(\"Model ROC Curves for Multi-class Classification\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 11 CNN-LSTM"
      ],
      "metadata": {
        "id": "M4OIesLwG2Jq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Output Directory name:\n",
        "output_dir = 'model_output/CNN-LSTM Part 2' # Store model's parameter after each epoch, useful at a later time\n",
        "\n",
        "# If the output_dir directory doesn't exist, we use the mkdirs() method to make it\n",
        "if not os.path.exists(output_dir):\n",
        "  os.makedirs(output_dir)\n",
        "\n",
        "# Allow to save our model parameters after each epoch during training\n",
        "modelcheckpoint = ModelCheckpoint(filepath = output_dir + '/weights.{epoch:02d}.hdf5')"
      ],
      "metadata": {
        "id": "r6yFr-TiN6qZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIk9HzcykZJc"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Embedding\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import SpatialDropout1D, Conv1D, GlobalMaxPooling1D # new!\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import os\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5pzTq0ikZJc"
      },
      "outputs": [],
      "source": [
        "# output directory name:\n",
        "output_dir = 'model_output/conv'\n",
        "\n",
        "drop_embed = 0.2 # new!\n",
        "# convolutional layer architecture:\n",
        "#n_conv = 256 # filters, a.k.a. kernels\n",
        "#k_conv = 3 # kernel length\n",
        "# dense layer architecture:\n",
        "n_lstm = 256\n",
        "drop_lstm = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4dru4F8kZJd"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, SpatialDropout1D, LSTM, Dense\n",
        "\n",
        "model11 = Sequential()\n",
        "model11.add(Embedding(n_unique_words, n_dim, input_length=max_message_length))\n",
        "model11.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "model11.add(MaxPooling1D(pool_size=2))\n",
        "model11.add(LSTM(100))\n",
        "model11.add(Dropout(dropout))\n",
        "model11.add(Dense(num_classes, activation='softmax'))\n",
        "model11.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Print the summary of the model\n",
        "model11.summary()\n",
        "\n",
        "# Fitting the model\n",
        "start_time = time.time()\n",
        "history11 = model11.fit(x_train, y_train_one_hot, validation_data=(x_test, y_test_one_hot), epochs=epochs, batch_size=batch_size, callbacks=[modelcheckpoint])\n",
        "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCD5cNRLJwVI"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5QSXNm7JwVJ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "# Visualizing training and validation accuracy for this Model\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history11.history['accuracy'])\n",
        "plt.plot(history11.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Visualizing training and validation loss for this Model\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history11.history['loss'])\n",
        "plt.plot(history11.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss/Error')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RP5epXg2JwVL"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Evaluating the model\n",
        "y_pred_prob = model11.predict(x_test)\n",
        "y_pred = y_pred_prob.argmax(axis=-1)\n",
        "\n",
        "# Calculating accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {100*accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9PZPc8CJwVM"
      },
      "outputs": [],
      "source": [
        "# Plot ROC curve for model\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(y_pred_prob)\n",
        "_ = plt.axvline(x=0.5, color='orange')\n",
        "plt.title(\"Model ROC-AUC\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAetrxByJwVN"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming y_test_encoded and y_pred_prob are defined\n",
        "# y_test_encoded: True labels (encoded, not one-hot encoded)\n",
        "# y_pred_prob: Predicted probabilities for each class\n",
        "\n",
        "# Calculate ROC-AUC score for model (one-vs-all approach for multi-class)\n",
        "roc_auc = []\n",
        "\n",
        "# Create a figure and axis\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot ROC curve for each class\n",
        "for i in range(num_classes):\n",
        "    fpr, tpr, _ = roc_curve(y_test== i, y_pred_prob[:, i])\n",
        "    roc_auc_class = auc(fpr, tpr)\n",
        "    roc_auc.append(roc_auc_class * 100.0)\n",
        "    plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc_class:.2f})')\n",
        "\n",
        "# Set plot title and labels\n",
        "plt.title(\"Model ROC Curves for Multi-class Classification\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 12 Dilated Convolutional Neural Network (DCNN)"
      ],
      "metadata": {
        "id": "659YWOYvKFjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Output Directory name:\n",
        "output_dir = 'model_output/DCNN' # Store model's parameter after each epoch, useful at a later time\n",
        "\n",
        "# If the output_dir directory doesn't exist, we use the mkdirs() method to make it\n",
        "if not os.path.exists(output_dir):\n",
        "  os.makedirs(output_dir)\n",
        "\n",
        "# Allow to save our model parameters after each epoch during training\n",
        "modelcheckpoint = ModelCheckpoint(filepath = output_dir + '/weights.{epoch:02d}.hdf5')"
      ],
      "metadata": {
        "id": "j2oUjzwlN_Bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dropout, Dense\n",
        "\n",
        "model12 = Sequential()\n",
        "model12.add(Embedding(n_unique_words, n_dim, input_length=max_message_length))\n",
        "model12.add(Conv1D(filters=64, kernel_size=3, dilation_rate=2, activation='relu'))\n",
        "model12.add(GlobalMaxPooling1D())\n",
        "model12.add(Dense(128, activation='relu'))\n",
        "model12.add(Dropout(0.5))\n",
        "model12.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model12.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model12.summary()\n",
        "\n",
        "start_time = time.time()\n",
        "history12 = model12.fit(x_train, y_train_one_hot, validation_data=(x_test, y_test_one_hot), epochs=epochs, batch_size=batch_size, callbacks=[modelcheckpoint])\n",
        "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed"
      ],
      "metadata": {
        "id": "9Yt-XsauG6Ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJknPD8hKeLJ"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83tX4s-mKeLd"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "# Visualizing training and validation accuracy for this Model\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history12.history['accuracy'])\n",
        "plt.plot(history12.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Visualizing training and validation loss for this Model\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history12.history['loss'])\n",
        "plt.plot(history12.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss/Error')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueYR_7zlKeLe"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Evaluating the model\n",
        "y_pred_prob = model12.predict(x_test)\n",
        "y_pred = y_pred_prob.argmax(axis=-1)\n",
        "\n",
        "# Calculating accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {100*accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wVUi5KoKeLf"
      },
      "outputs": [],
      "source": [
        "# Plot ROC curve for model\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(y_pred_prob)\n",
        "_ = plt.axvline(x=0.5, color='orange')\n",
        "plt.title(\"Model ROC-AUC\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpTTpBZHKeLf"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming y_test_encoded and y_pred_prob are defined\n",
        "# y_test_encoded: True labels (encoded, not one-hot encoded)\n",
        "# y_pred_prob: Predicted probabilities for each class\n",
        "\n",
        "# Calculate ROC-AUC score for model (one-vs-all approach for multi-class)\n",
        "roc_auc = []\n",
        "\n",
        "# Create a figure and axis\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot ROC curve for each class\n",
        "for i in range(num_classes):\n",
        "    fpr, tpr, _ = roc_curve(y_test== i, y_pred_prob[:, i])\n",
        "    roc_auc_class = auc(fpr, tpr)\n",
        "    roc_auc.append(roc_auc_class * 100.0)\n",
        "    plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc_class:.2f})')\n",
        "\n",
        "# Set plot title and labels\n",
        "plt.title(\"Model ROC Curves for Multi-class Classification\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 13 Residual Network (ResNet)"
      ],
      "metadata": {
        "id": "yIn008NWKnYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Output Directory name:\n",
        "output_dir = 'model_output/Resnet' # Store model's parameter after each epoch, useful at a later time\n",
        "\n",
        "# If the output_dir directory doesn't exist, we use the mkdirs() method to make it\n",
        "if not os.path.exists(output_dir):\n",
        "  os.makedirs(output_dir)\n",
        "\n",
        "# Allow to save our model parameters after each epoch during training\n",
        "modelcheckpoint = ModelCheckpoint(filepath = output_dir + '/weights.{epoch:02d}.hdf5')"
      ],
      "metadata": {
        "id": "Ir7zMuAqOBEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalAveragePooling1D, Dense\n",
        "\n",
        "model13 = Sequential()\n",
        "model13.add(Embedding(n_unique_words, n_dim, input_length=max_message_length))\n",
        "model13.add(Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'))\n",
        "model13.add(Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'))\n",
        "model13.add(MaxPooling1D(pool_size=2))\n",
        "model13.add(Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'))\n",
        "model13.add(Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'))\n",
        "model13.add(MaxPooling1D(pool_size=2))\n",
        "model13.add(GlobalAveragePooling1D())\n",
        "model13.add(Dense(128, activation='relu'))\n",
        "model13.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model13.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model13.summary()\n",
        "\n",
        "start_time = time.time()\n",
        "history13 = model13.fit(x_train, y_train_one_hot, validation_data=(x_test, y_test_one_hot), epochs=epochs, batch_size=batch_size, callbacks=[modelcheckpoint])\n",
        "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed"
      ],
      "metadata": {
        "id": "clULgp2gKrbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBIUzQKeLB4y"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yusHvwmJLB5B"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "# Visualizing training and validation accuracy for this Model\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history13.history['accuracy'])\n",
        "plt.plot(history13.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Visualizing training and validation loss for this Model\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history13.history['loss'])\n",
        "plt.plot(history13.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss/Error')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKEJdS2KLB5C"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Evaluating the model\n",
        "y_pred_prob = model13.predict(x_test)\n",
        "y_pred = y_pred_prob.argmax(axis=-1)\n",
        "\n",
        "# Calculating accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {100*accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcCBGKP3LB5C"
      },
      "outputs": [],
      "source": [
        "# Plot ROC curve for model\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(y_pred_prob)\n",
        "_ = plt.axvline(x=0.5, color='orange')\n",
        "plt.title(\"Model ROC-AUC\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DhZAB-xLB5D"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming y_test_encoded and y_pred_prob are defined\n",
        "# y_test_encoded: True labels (encoded, not one-hot encoded)\n",
        "# y_pred_prob: Predicted probabilities for each class\n",
        "\n",
        "# Calculate ROC-AUC score for model (one-vs-all approach for multi-class)\n",
        "roc_auc = []\n",
        "\n",
        "# Create a figure and axis\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot ROC curve for each class\n",
        "for i in range(num_classes):\n",
        "    fpr, tpr, _ = roc_curve(y_test== i, y_pred_prob[:, i])\n",
        "    roc_auc_class = auc(fpr, tpr)\n",
        "    roc_auc.append(roc_auc_class * 100.0)\n",
        "    plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc_class:.2f})')\n",
        "\n",
        "# Set plot title and labels\n",
        "plt.title(\"Model ROC Curves for Multi-class Classification\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 14 Variational autoencoder (VAE) Network"
      ],
      "metadata": {
        "id": "9IpK27KiLGnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Output Directory name:\n",
        "output_dir = 'model_output/VAE' # Store model's parameter after each epoch, useful at a later time\n",
        "\n",
        "# If the output_dir directory doesn't exist, we use the mkdirs() method to make it\n",
        "if not os.path.exists(output_dir):\n",
        "  os.makedirs(output_dir)\n",
        "\n",
        "# Allow to save our model parameters after each epoch during training\n",
        "modelcheckpoint = ModelCheckpoint(filepath = output_dir + '/weights.{epoch:02d}.hdf5')"
      ],
      "metadata": {
        "id": "C9rCowZTODZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense, Lambda, Input\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "\n",
        "# Define the input layer\n",
        "input_layer = Input(shape=(max_message_length,))\n",
        "\n",
        "# Embedding layer\n",
        "embedding_layer = Embedding(input_dim=n_unique_words, output_dim=n_dim, input_length=max_message_length)(input_layer)\n",
        "\n",
        "# Flatten the embedding output\n",
        "flatten_layer = Flatten()(embedding_layer)\n",
        "\n",
        "# Intermediate dense layers\n",
        "hidden_layer_1 = Dense(128, activation='relu')(flatten_layer)\n",
        "hidden_layer_2 = Dense(64, activation='relu')(hidden_layer_1)\n",
        "\n",
        "# Output layers for mean and log variance\n",
        "z_mean = Dense(32, name='z_mean')(hidden_layer_2)\n",
        "z_log_var = Dense(32, name='z_log_var')(hidden_layer_2)\n",
        "\n",
        "# Sampling function\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    batch = K.shape(z_mean)[0]\n",
        "    dim = K.int_shape(z_mean)[1]\n",
        "    epsilon = K.random_normal(shape=(batch, dim))\n",
        "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "# Create a Lambda layer for the sampling function\n",
        "z = Lambda(sampling, output_shape=(32,), name='z')([z_mean, z_log_var])\n",
        "\n",
        "# Dense layer with softmax activation for multiclass classification\n",
        "output_layer = Dense(num_classes, activation='softmax', name='output')(z)\n",
        "\n",
        "# Create a new model with input and output layers\n",
        "model14 = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compile the model\n",
        "model14.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Display the model summary\n",
        "model14.summary()\n",
        "\n",
        "start_time = time.time()\n",
        "history14 = model14.fit(x_train, y_train_one_hot, validation_data=(x_test, y_test_one_hot), epochs=epochs, batch_size=batch_size, callbacks=[modelcheckpoint])\n",
        "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed"
      ],
      "metadata": {
        "id": "VPxrDL0PLMHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIGQlHOiLki8"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWs3KNfuLki9"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "# Visualizing training and validation accuracy for this Model\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history14.history['accuracy'])\n",
        "plt.plot(history14.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Visualizing training and validation loss for this Model\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history14.history['loss'])\n",
        "plt.plot(history14.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss/Error')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlJl9KzvLki9"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Evaluating the model\n",
        "y_pred_prob = model14.predict(x_test)\n",
        "y_pred = y_pred_prob.argmax(axis=-1)\n",
        "\n",
        "# Calculating accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {100*accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDloo5B0Lki-"
      },
      "outputs": [],
      "source": [
        "# Plot ROC curve for model\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(y_pred_prob)\n",
        "_ = plt.axvline(x=0.5, color='orange')\n",
        "plt.title(\"Model ROC-AUC\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwVqFANMLki-"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming y_test_encoded and y_pred_prob are defined\n",
        "# y_test_encoded: True labels (encoded, not one-hot encoded)\n",
        "# y_pred_prob: Predicted probabilities for each class\n",
        "\n",
        "# Calculate ROC-AUC score for model (one-vs-all approach for multi-class)\n",
        "roc_auc = []\n",
        "\n",
        "# Create a figure and axis\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot ROC curve for each class\n",
        "for i in range(num_classes):\n",
        "    fpr, tpr, _ = roc_curve(y_test== i, y_pred_prob[:, i])\n",
        "    roc_auc_class = auc(fpr, tpr)\n",
        "    roc_auc.append(roc_auc_class * 100.0)\n",
        "    plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc_class:.2f})')\n",
        "\n",
        "# Set plot title and labels\n",
        "plt.title(\"Model ROC Curves for Multi-class Classification\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 15 Bi-GRU"
      ],
      "metadata": {
        "id": "c3TrCGZELuqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Output Directory name:\n",
        "output_dir = 'model_output/Bi-GRU' # Store model's parameter after each epoch, useful at a later time\n",
        "\n",
        "# If the output_dir directory doesn't exist, we use the mkdirs() method to make it\n",
        "if not os.path.exists(output_dir):\n",
        "  os.makedirs(output_dir)\n",
        "\n",
        "# Allow to save our model parameters after each epoch during training\n",
        "modelcheckpoint = ModelCheckpoint(filepath = output_dir + '/weights.{epoch:02d}.hdf5')"
      ],
      "metadata": {
        "id": "nfjvS8eJOF2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Bidirectional, GRU, Dense\n",
        "\n",
        "model15 = Sequential()\n",
        "model15.add(Embedding(n_unique_words, n_dim, input_length=max_message_length))\n",
        "model15.add(Bidirectional(GRU(64)))\n",
        "model15.add(Dense(64, activation='relu'))\n",
        "model15.add(Dense(num_classes, activation='softmax'))\n",
        "model15.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model15.summary()\n",
        "\n",
        "start_time = time.time()\n",
        "history15 = model15.fit(x_train, y_train_one_hot, validation_data=(x_test, y_test_one_hot), epochs=epochs, batch_size=batch_size, callbacks=[modelcheckpoint])\n",
        "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed"
      ],
      "metadata": {
        "id": "guykHhQrLrfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OBw3KhGMIFN"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BowDGQqiMIFP"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "# Visualizing training and validation accuracy for this Model\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history15.history['accuracy'])\n",
        "plt.plot(history15.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Visualizing training and validation loss for this Model\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history15.history['loss'])\n",
        "plt.plot(history15.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss/Error')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55s4SvzfMIFQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Evaluating the model\n",
        "y_pred_prob = model15.predict(x_test)\n",
        "y_pred = y_pred_prob.argmax(axis=-1)\n",
        "\n",
        "# Calculating accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {100*accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Va2ABZLRMIFR"
      },
      "outputs": [],
      "source": [
        "# Plot ROC curve for model\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(y_pred_prob)\n",
        "_ = plt.axvline(x=0.5, color='orange')\n",
        "plt.title(\"Model ROC-AUC\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwMbRMaaMIFS"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming y_test_encoded and y_pred_prob are defined\n",
        "# y_test_encoded: True labels (encoded, not one-hot encoded)\n",
        "# y_pred_prob: Predicted probabilities for each class\n",
        "\n",
        "# Calculate ROC-AUC score for model (one-vs-all approach for multi-class)\n",
        "roc_auc = []\n",
        "\n",
        "# Create a figure and axis\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot ROC curve for each class\n",
        "for i in range(num_classes):\n",
        "    fpr, tpr, _ = roc_curve(y_test== i, y_pred_prob[:, i])\n",
        "    roc_auc_class = auc(fpr, tpr)\n",
        "    roc_auc.append(roc_auc_class * 100.0)\n",
        "    plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc_class:.2f})')\n",
        "\n",
        "# Set plot title and labels\n",
        "plt.title(\"Model ROC Curves for Multi-class Classification\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 16 Multi-Layer Perceptron (MLP)"
      ],
      "metadata": {
        "id": "LiKYGt3HMOkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Output Directory name:\n",
        "output_dir = 'model_output/MLP' # Store model's parameter after each epoch, useful at a later time\n",
        "\n",
        "# If the output_dir directory doesn't exist, we use the mkdirs() method to make it\n",
        "if not os.path.exists(output_dir):\n",
        "  os.makedirs(output_dir)\n",
        "\n",
        "# Allow to save our model parameters after each epoch during training\n",
        "modelcheckpoint = ModelCheckpoint(filepath = output_dir + '/weights.{epoch:02d}.hdf5')"
      ],
      "metadata": {
        "id": "fPn1saaCOIEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten\n",
        "\n",
        "model16 = Sequential()\n",
        "model16.add(Dense(512, activation='relu', input_shape=(100,)))\n",
        "model16.add(Dense(256, activation='relu'))\n",
        "model16.add(Dense(128, activation='relu'))\n",
        "model16.add(Dense(64, activation='relu'))\n",
        "model16.add(Dense(32, activation='relu'))\n",
        "model16.add(Dense(num_classes, activation='softmax'))\n",
        "model16.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model16.summary()\n",
        "\n",
        "start_time = time.time()\n",
        "history16 = model16.fit(x_train, y_train_one_hot, validation_data=(x_test, y_test_one_hot), epochs=epochs, batch_size=batch_size, callbacks=[modelcheckpoint])\n",
        "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed"
      ],
      "metadata": {
        "id": "NYifJHy8LrcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uX_85m0bMnph"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_2H7DArMnpj"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "# Visualizing training and validation accuracy for this Model\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history16.history['accuracy'])\n",
        "plt.plot(history16.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Visualizing training and validation loss for this Model\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history16.history['loss'])\n",
        "plt.plot(history16.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss/Error')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_-KG1aKMnpk"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Evaluating the model\n",
        "y_pred_prob = model16.predict(x_test)\n",
        "y_pred = y_pred_prob.argmax(axis=-1)\n",
        "\n",
        "# Calculating accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {100*accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3t82MPoMnpl"
      },
      "outputs": [],
      "source": [
        "# Plot ROC curve for model\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(y_pred_prob)\n",
        "_ = plt.axvline(x=0.5, color='orange')\n",
        "plt.title(\"Model ROC-AUC\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLWfIIelMnpm"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming y_test_encoded and y_pred_prob are defined\n",
        "# y_test_encoded: True labels (encoded, not one-hot encoded)\n",
        "# y_pred_prob: Predicted probabilities for each class\n",
        "\n",
        "# Calculate ROC-AUC score for model (one-vs-all approach for multi-class)\n",
        "roc_auc = []\n",
        "\n",
        "# Create a figure and axis\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot ROC curve for each class\n",
        "for i in range(num_classes):\n",
        "    fpr, tpr, _ = roc_curve(y_test== i, y_pred_prob[:, i])\n",
        "    roc_auc_class = auc(fpr, tpr)\n",
        "    roc_auc.append(roc_auc_class * 100.0)\n",
        "    plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc_class:.2f})')\n",
        "\n",
        "# Set plot title and labels\n",
        "plt.title(\"Model ROC Curves for Multi-class Classification\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wBG1DoV2Mw0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 17 CNN-GRU"
      ],
      "metadata": {
        "id": "VumfwSkiazMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Output Directory name:\n",
        "output_dir = 'model_output/CNN-GRU' # Store model's parameter after each epoch, useful at a later time\n",
        "\n",
        "# If the output_dir directory doesn't exist, we use the mkdirs() method to make it\n",
        "if not os.path.exists(output_dir):\n",
        "  os.makedirs(output_dir)\n",
        "\n",
        "# Allow to save our model parameters after each epoch during training\n",
        "modelcheckpoint = ModelCheckpoint(filepath = output_dir + '/weights.{epoch:02d}.hdf5')"
      ],
      "metadata": {
        "id": "UsI4-JYEazM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Conv1D, MaxPooling1D, GRU, Dense, Dropout, Flatten\n",
        "\n",
        "\n",
        "model17 = Sequential()\n",
        "\n",
        "# Embedding layer\n",
        "model17.add(Embedding(n_unique_words, n_dim, input_length=max_message_length))\n",
        "\n",
        "# Convolutional layers\n",
        "model17.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "model17.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "# GRU layer\n",
        "model17.add(GRU(100, dropout=dropout, recurrent_dropout=dropout))\n",
        "\n",
        "# Fully connected layers\n",
        "model17.add(Dense(64, activation='relu'))\n",
        "model17.add(Dropout(dropout))\n",
        "\n",
        "# Output layer\n",
        "model17.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model17.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Print the summary of the model\n",
        "model17.summary()\n",
        "\n",
        "start_time = time.time()\n",
        "history17 = model17.fit(x_train, y_train_one_hot, validation_data=(x_test, y_test_one_hot), epochs=epochs, batch_size=batch_size, callbacks=[modelcheckpoint])\n",
        "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed"
      ],
      "metadata": {
        "id": "9nmPl0kVazM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "loss17, accuracy17 = model17.evaluate(x_test, y_test_one_hot)\n",
        "print(f'Test Loss: {loss17}, Test Accuracy: {100*accuracy17:.2f}')"
      ],
      "metadata": {
        "id": "s0eOH1ZFcMGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72LdVXheazM0"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKm3LapFazM1"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "# Visualizing training and validation accuracy for this Model\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history17.history['accuracy'])\n",
        "plt.plot(history17.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Visualizing training and validation loss for this Model\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history17.history['loss'])\n",
        "plt.plot(history17.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss/Error')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oiSwhZQlazM1"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Evaluating the model\n",
        "y_pred_prob = model17.predict(x_test)\n",
        "y_pred = y_pred_prob.argmax(axis=-1)\n",
        "\n",
        "\n",
        "# Calculating accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {100*accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayeJI-i2azM1"
      },
      "outputs": [],
      "source": [
        "# Plot ROC curve for model\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(y_pred_prob)\n",
        "_ = plt.axvline(x=0.5, color='orange')\n",
        "plt.title(\"Model ROC-AUC\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHe_9Vz7azM1"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming y_test_encoded and y_pred_prob are defined\n",
        "# y_test_encoded: True labels (encoded, not one-hot encoded)\n",
        "# y_pred_prob: Predicted probabilities for each class\n",
        "\n",
        "# Calculate ROC-AUC score for model (one-vs-all approach for multi-class)\n",
        "roc_auc = []\n",
        "\n",
        "# Create a figure and axis\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot ROC curve for each class\n",
        "for i in range(num_classes):\n",
        "    fpr, tpr, _ = roc_curve(y_test== i, y_pred_prob[:, i])\n",
        "    roc_auc_class = auc(fpr, tpr)\n",
        "    roc_auc.append(roc_auc_class * 100.0)\n",
        "    plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc_class:.2f})')\n",
        "\n",
        "# Set plot title and labels\n",
        "plt.title(\"Model ROC Curves for Multi-class Classification\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 18 CNN + Bi-GRU"
      ],
      "metadata": {
        "id": "RujsqG8Na9aN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Output Directory name:\n",
        "output_dir = 'model_output/CNN-Bi-GRU' # Store model's parameter after each epoch, useful at a later time\n",
        "\n",
        "# If the output_dir directory doesn't exist, we use the mkdirs() method to make it\n",
        "if not os.path.exists(output_dir):\n",
        "  os.makedirs(output_dir)\n",
        "\n",
        "# Allow to save our model parameters after each epoch during training\n",
        "modelcheckpoint = ModelCheckpoint(filepath = output_dir + '/weights.{epoch:02d}.hdf5')"
      ],
      "metadata": {
        "id": "wEj2EMBUa9aN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, GRU, Dense, Dropout, Flatten\n",
        "\n",
        "\n",
        "model18 = Sequential()\n",
        "\n",
        "# Embedding layer\n",
        "model18.add(Embedding(n_unique_words, n_dim, input_length=max_message_length))\n",
        "\n",
        "# Convolutional layers\n",
        "model18.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "model18.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "# Bidirectional GRU layer\n",
        "model18.add(Bidirectional(GRU(100, dropout=dropout, recurrent_dropout=dropout)))\n",
        "\n",
        "# Fully connected layers\n",
        "model18.add(Dense(64, activation='relu'))\n",
        "model18.add(Dropout(dropout))\n",
        "\n",
        "# Output layer\n",
        "model18.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model18.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Print the summary of the model\n",
        "model18.summary()\n",
        "\n",
        "start_time = time.time()\n",
        "history18 = model18.fit(x_train, y_train_one_hot, validation_data=(x_test, y_test_one_hot), epochs=epochs, batch_size=batch_size, callbacks=[modelcheckpoint])\n",
        "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed"
      ],
      "metadata": {
        "id": "XkMHiInwa9aO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "loss18, accuracy18 = model18.evaluate(x_test, y_test_one_hot)\n",
        "print(f'Test Loss: {loss18}, Test Accuracy: {100*accuracy18:.2f}')"
      ],
      "metadata": {
        "id": "i9_AOZUnhuBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpD9mfmBa9aO"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOUQ7jFVa9aO"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "# Visualizing training and validation accuracy for this Model\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history18.history['accuracy'])\n",
        "plt.plot(history18.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Visualizing training and validation loss for this Model\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history18.history['loss'])\n",
        "plt.plot(history18.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss/Error')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpQIm5vCa9aO"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Evaluating the model\n",
        "y_pred_prob = model18.predict(x_test)\n",
        "y_pred = y_pred_prob.argmax(axis=-1)\n",
        "\n",
        "# Calculating accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {100*accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHnK1Ydsa9aP"
      },
      "outputs": [],
      "source": [
        "# Plot ROC curve for model\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(y_pred_prob)\n",
        "_ = plt.axvline(x=0.5, color='orange')\n",
        "plt.title(\"Model ROC-AUC\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvxSDq4ea9aP"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming y_test_encoded and y_pred_prob are defined\n",
        "# y_test_encoded: True labels (encoded, not one-hot encoded)\n",
        "# y_pred_prob: Predicted probabilities for each class\n",
        "\n",
        "# Calculate ROC-AUC score for model (one-vs-all approach for multi-class)\n",
        "roc_auc = []\n",
        "\n",
        "# Create a figure and axis\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot ROC curve for each class\n",
        "for i in range(num_classes):\n",
        "    fpr, tpr, _ = roc_curve(y_test== i, y_pred_prob[:, i])\n",
        "    roc_auc_class = auc(fpr, tpr)\n",
        "    roc_auc.append(roc_auc_class * 100.0)\n",
        "    plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc_class:.2f})')\n",
        "\n",
        "# Set plot title and labels\n",
        "plt.title(\"Model ROC Curves for Multi-class Classification\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 19 CNN + Bi-LSTM"
      ],
      "metadata": {
        "id": "bUi6GgPmbD3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Output Directory name:\n",
        "output_dir = 'model_output/CNN-Bi-LSTM' # Store model's parameter after each epoch, useful at a later time\n",
        "\n",
        "# If the output_dir directory doesn't exist, we use the mkdirs() method to make it\n",
        "if not os.path.exists(output_dir):\n",
        "  os.makedirs(output_dir)\n",
        "\n",
        "# Allow to save our model parameters after each epoch during training\n",
        "modelcheckpoint = ModelCheckpoint(filepath = output_dir + '/weights.{epoch:02d}.hdf5')"
      ],
      "metadata": {
        "id": "LnaUwDWnbMZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout, Flatten\n",
        "\n",
        "\n",
        "model19 = Sequential()\n",
        "\n",
        "# Embedding layer\n",
        "model19.add(Embedding(n_unique_words, n_dim, input_length=max_message_length))\n",
        "\n",
        "# Convolutional layers\n",
        "model19.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "model19.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "# Bidirectional LSTM layer\n",
        "model19.add(Bidirectional(LSTM(100, dropout=dropout, recurrent_dropout=dropout)))\n",
        "\n",
        "# Fully connected layers\n",
        "model19.add(Dense(64, activation='relu'))\n",
        "model19.add(Dropout(dropout))\n",
        "\n",
        "# Output layer\n",
        "model19.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model19.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Print the summary of the model\n",
        "model19.summary()\n",
        "\n",
        "start_time = time.time()\n",
        "history19 = model19.fit(x_train, y_train_one_hot, validation_data=(x_test, y_test_one_hot), epochs=epochs, batch_size=batch_size, callbacks=[modelcheckpoint])\n",
        "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed"
      ],
      "metadata": {
        "id": "At_eS4WBbMZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "loss19, accuracy19 = model19.evaluate(x_test, y_test_one_hot)\n",
        "print(f'Test Loss: {loss19}, Test Accuracy: {100*accuracy19:.2f}')"
      ],
      "metadata": {
        "id": "rO6Iqz0ehzLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUNOieE8bMZQ"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceZcHgV2bMZQ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "# Visualizing training and validation accuracy for this Model\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history19.history['accuracy'])\n",
        "plt.plot(history19.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Visualizing training and validation loss for this Model\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history19.history['loss'])\n",
        "plt.plot(history19.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss/Error')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOQoJlvwbMZQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Evaluating the model\n",
        "y_pred_prob = model19.predict(x_test)\n",
        "y_pred = y_pred_prob.argmax(axis=-1)\n",
        "\n",
        "# Calculating accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {100*accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRAOR6lcbMZR"
      },
      "outputs": [],
      "source": [
        "# Plot ROC curve for model\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(y_pred_prob)\n",
        "_ = plt.axvline(x=0.5, color='orange')\n",
        "plt.title(\"Model ROC-AUC\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tr0o3iygbMZR"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming y_test_encoded and y_pred_prob are defined\n",
        "# y_test_encoded: True labels (encoded, not one-hot encoded)\n",
        "# y_pred_prob: Predicted probabilities for each class\n",
        "\n",
        "# Calculate ROC-AUC score for model (one-vs-all approach for multi-class)\n",
        "roc_auc = []\n",
        "\n",
        "# Create a figure and axis\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot ROC curve for each class\n",
        "for i in range(num_classes):\n",
        "    fpr, tpr, _ = roc_curve(y_test== i, y_pred_prob[:, i])\n",
        "    roc_auc_class = auc(fpr, tpr)\n",
        "    roc_auc.append(roc_auc_class * 100.0)\n",
        "    plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc_class:.2f})')\n",
        "\n",
        "# Set plot title and labels\n",
        "plt.title(\"Model ROC Curves for Multi-class Classification\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZsPvZzxiLrZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2NcZfvDooX9"
      },
      "source": [
        "## Model 20 BERT Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Output Directory name:\n",
        "output_dir = 'model_output/BERT' # Store model's parameter after each epoch, useful at a later time\n",
        "\n",
        "# If the output_dir directory doesn't exist, we use the mkdirs() method to make it\n",
        "if not os.path.exists(output_dir):\n",
        "  os.makedirs(output_dir)\n",
        "\n",
        "# Allow to save our model parameters after each epoch during training\n",
        "modelcheckpoint = ModelCheckpoint(filepath = output_dir + '/weights.{epoch:02d}.hdf5')"
      ],
      "metadata": {
        "id": "SBaeYCW2OMj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWX-cPUgPWrS"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train_for_tokens, X_val_for_tokens, y_train_for_tokens, y_val_for_tokens = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KG90e2EYbbsY"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the training data\n",
        "train_tokens = tokenizer(\n",
        "    X_train_for_tokens.tolist(),\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=128,  # Adjust as needed\n",
        "    return_tensors='tf',  # Use 'tf' for TensorFlow\n",
        ")\n",
        "\n",
        "# Tokenize the validation data\n",
        "val_tokens = tokenizer(\n",
        "    X_val_for_tokens.tolist(),\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=128,  # Adjust as needed\n",
        "    return_tensors='tf',  # Use 'tf' for TensorFlow\n",
        ")\n",
        "\n",
        "# Display the tokenized input\n",
        "print(train_tokens)\n",
        "print(val_tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kG5VLF6TQSZD"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Load the BERT model with pooler layer set to non-trainable\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-uncased', trainable=False)\n",
        "\n",
        "# Define the input layer\n",
        "input_layer = Input(shape=(128,), dtype=tf.int32, name='input_ids')\n",
        "\n",
        "# Connect the input layer to the BERT model\n",
        "bert_output = bert_model(input_layer)[0]\n",
        "\n",
        "# Global average pooling\n",
        "pooled_output = tf.reduce_mean(bert_output, axis=1)\n",
        "\n",
        "# Add a dense layer for classification\n",
        "dense_layer = Dense(256, activation='relu')(pooled_output)\n",
        "\n",
        "# Output layer for multiclass classification\n",
        "output_layer = Dense(num_classes, activation='softmax')(dense_layer)\n",
        "\n",
        "# Create the model\n",
        "model_bert = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compile the model\n",
        "model_bert.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Display the model summary\n",
        "model_bert.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xb-BROXVzMye"
      },
      "outputs": [],
      "source": [
        "# Convert the labels to NumPy arrays and one-hot encode them\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "y_train_for_tokens_encoded = encoder.fit_transform(y_train_for_tokens.values.reshape(-1, 1))\n",
        "y_val_for_tokens_encoded = encoder.transform(y_val_for_tokens.values.reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VfIoRqXxzbt"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "start_time = time.time()\n",
        "history_model_bert = model_bert.fit(\n",
        "    train_tokens['input_ids'],\n",
        "    y_train_for_tokens_encoded,\n",
        "    validation_data=(val_tokens['input_ids'], y_val_for_tokens_encoded),\n",
        "    epochs=2,  # Adjust as needed\n",
        "    batch_size=256,  # Adjust as needed\n",
        "    callbacks=[modelcheckpoint]\n",
        ")\n",
        "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvqSoUhaooX-"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "# Visualizing training and validation accuracy for this Model\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_model_bert.history['accuracy'])\n",
        "plt.plot(history_model_bert.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Visualizing training and validation loss for this Model\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_model_bert.history['loss'])\n",
        "plt.plot(history_model_bert.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss/Error')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ]
}